{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584bcad6-7a46-4606-8b70-13c2b8665962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8019c8a2-9c70-4aef-9006-4da0448b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    # img = std * img + mean  # unnormalize\n",
    "    # img = np.clip(img, 0, 1)  # clip any values outside the range [0, 1]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd6341b-1f94-4224-98e5-193955015849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_vs_reconstructed_images(images, reconstructed_imgs):\n",
    "    # Move images back to cpu for visualization\n",
    "    images = images.cpu()\n",
    "    reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "    \n",
    "    # Display original and reconstructed images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original Images')\n",
    "    imshow(vutils.make_grid(images, padding=2, normalize=True))\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Reconstructed Images')\n",
    "    imshow(vutils.make_grid(reconstructed_imgs, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2985d0-8e80-4209-8e6a-62013be56d72",
   "metadata": {},
   "source": [
    "### AutoEncoder Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5710f5f-39a9-448c-bac3-b455e17ad667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHANGE\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(8192, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "encoder = Encoder(num_input_channels=3, base_channel_size = 64, latent_dim = 2048)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856d6054-ff8b-49dc-a6fd-9a48f979423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 8192), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 16x16 => 32x32\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], 128, 8, 8)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(num_input_channels=3, base_channel_size = 64, latent_dim = 2048)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38582019-41b7-4ff8-ad20-033e2d55b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder = autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1980ad73-6d1e-4b59-99ce-7cebeaceaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_chckpt = torch.load('deep_autoencoder_v1_40kloss.pth')\n",
    "# print(autoencoder_chckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6eaccf-6473-45b6-b878-d2599061ec11",
   "metadata": {},
   "source": [
    "deep_encoder_checkpoint_epoch_21 fot on the EncoderAndLabels nb, loss on train of 0.0087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18f710e4-04f1-4bb6-b083-00ddc44f6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(f'deep_encoder_checkpoint_epoch_21.pth')['encoder_state_dict'])\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6c847d-289b-4efe-9a12-0bcbea951a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.load_state_dict(autoencoder_chckpt['encoder_state_dict'])\n",
    "# decoder.load_state_dict(autoencoder_chckpt['decoder_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca44b04-2c5f-4793-8262-73ac1bd02a99",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6413136e-3786-4abf-a69c-18fdd0c0dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13000, 2); Valid: (2000, 2); Test: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "valid_df = pd.read_csv('dataset/val.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "print(f\"Train: {train_df.shape}; Valid: {valid_df.shape}; Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db7cce9b-b1eb-49ed-88d2-3926c6d8b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Image', 'Class'], dtype='object'),\n",
       "                                       Image  Class\n",
       " 0  0be195e0-eb16-4f29-ac7c-196dec9da47d.png     79\n",
       " 1  28045419-b3b2-415b-9085-b4d241944235.png     94\n",
       " 2  b7078f35-d239-4dd6-babb-1af7be1b9364.png     79\n",
       " 3  0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png     23\n",
       " 4  ba11dda2-37d7-4d28-8bbb-128d452a171c.png     88)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3061b5ab-5947-4a11-b3a3-4890623ab9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train_images/'\n",
    "val_path = 'dataset/val_images/'\n",
    "test_path = 'dataset/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e2df1c7-3abe-4f99-9785-7edc8fbe02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png\n"
     ]
    }
   ],
   "source": [
    "train_image_path = os.path.join(train_path,train_df['Image'][0])\n",
    "print(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc4c483c-1fe0-4c3b-b2ef-3cb4f9dc5b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_image_path)\n",
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97c12f1f-870d-4911-84cc-971f241a8bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png', 'dataset/train_images/28045419-b3b2-415b-9085-b4d241944235.png', 'dataset/train_images/b7078f35-d239-4dd6-babb-1af7be1b9364.png', 'dataset/train_images/0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png', 'dataset/train_images/ba11dda2-37d7-4d28-8bbb-128d452a171c.png'] [79, 94, 79, 23, 88]\n"
     ]
    }
   ],
   "source": [
    "train_image_names = train_df['Image'].tolist()\n",
    "train_image_labels = train_df['Class'].tolist()\n",
    "# print(train_image_names[:5], train_image_labels[:5])\n",
    "\n",
    "train_image_paths = [os.path.join(train_path, image_name)for image_name in train_image_names]\n",
    "print(train_image_paths[:5], train_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2b59680-8413-4ac5-b557-54b507d7e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/val_images/e91a8fbc-d3ba-4b39-8c2f-04c14de78e5e.png', 'dataset/val_images/7c40819b-c3ce-4a91-9e98-c3df11b63623.png', 'dataset/val_images/d54269d7-fe86-4112-9c0f-99cc6ab8d9c0.png', 'dataset/val_images/cbf9ac9e-0859-4b54-ae65-347587b45deb.png', 'dataset/val_images/6aafce3f-9002-44e0-9a99-ffe9b49c9bac.png'] [32, 85, 41, 97, 62]\n"
     ]
    }
   ],
   "source": [
    "val_image_names = valid_df['Image'].tolist()\n",
    "val_image_labels = valid_df['Class'].tolist()\n",
    "\n",
    "val_image_paths = [os.path.join(val_path, image_name)for image_name in val_image_names]\n",
    "print(val_image_paths[:5], val_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "483a54d3-0b6c-4bbc-b5d9-6a9595fdf223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/test_images/046f61c4-b825-459a-8b2d-07503f5b94a5.png', 'dataset/test_images/67db001f-e287-4950-ac49-6683b493d1a4.png', 'dataset/test_images/9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png', 'dataset/test_images/5ffef91a-aaf9-4d0d-a219-83a9f5282361.png', 'dataset/test_images/c00af570-0000-4f8f-a3f2-c37a981bfdb1.png']\n"
     ]
    }
   ],
   "source": [
    "test_image_names = test_df['Image'].tolist()\n",
    "test_image_paths = [os.path.join(test_path, image_name)for image_name in test_image_names]\n",
    "print(test_image_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac1c64e2-5db6-4d37-92a9-dd104dfd3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transform = T.Compose([\n",
    "    # T.Resize((64, 64)), # -> all are already 64 * 64\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.42835271, 0.40658227, 0.34071648], [0.2144312,  0.21884131, 0.20464434])\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f81218bf-abca-4a3a-a61c-c8789e06d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process an image and get embedding\n",
    "def get_embedding(image_path, encoder, flatten=True):\n",
    "    image = Image.open(image_path)\n",
    "    image = basic_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = encoder(image)\n",
    "    \n",
    "    if flatten:\n",
    "        # Flatten the embedding\n",
    "        embedding = torch.flatten(embedding, start_dim=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57ac49db-3e0a-4f3b-a4e1-dca15fa9689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embedding(os.path.join(train_path, train_image_names[0]), encoder, flatten=False)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557463a5-c197-4b32-b9a8-bae47d3b12c1",
   "metadata": {},
   "source": [
    "### Train, Valid, Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ed5a6e3-4a80-4f4b-babc-ea018c7b62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1ace968-ae8b-40b6-8e4d-438a0308a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04471800-8fca-4ac4-9e74-26017d4acef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_image_paths, train_image_labels, transform = basic_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "221a2fbc-ebd4-491a-b8d8-10f1109d848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(val_image_paths, val_image_labels, transform = basic_transform)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d9b9a59-bf47-4f41-9d1d-e300586772b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_image_paths, transform = basic_transform)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5365e-79c9-41bf-8318-7c94b7b07ae6",
   "metadata": {},
   "source": [
    "### Encoder with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbeb6c07-c590-4ae0-8863-79046dfa0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderWithClassifier(nn.Module):\n",
    "#     def __init__(self, pretrained_encoder, num_classes, act_fn=nn.GELU):\n",
    "#         super().__init__()\n",
    "#         self.encoder = pretrained_encoder\n",
    "#         # Freeze the encoder\n",
    "#         for param in self.encoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         latent_dim = self.encoder.net[-1].out_features\n",
    "#         self.classifier = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder.net(x)  # Use the pre-trained encoder\n",
    "#         x = self.classifier(x)  # Classifier layer\n",
    "#         return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()  # Non-in-place version\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_features != out_features:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.BatchNorm1d(out_features)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.fc(x))  # Apply ReLU without in-place operation\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderWithClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_encoder, num_classes, act_fn=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_encoder\n",
    "        # Freeze the encoder\n",
    "        # for param in self.encoder.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        latent_dim = self.encoder.net[-1].out_features\n",
    "        self.act_fn = act_fn()  # Instantiate the activation function\n",
    "\n",
    "        # Intermediate layers\n",
    "        self.fc1 = nn.Linear(latent_dim, 1024)\n",
    "        self.fc2 = ResNetBlock(1024, 512)\n",
    "        self.fc3 = ResNetBlock(512, 256)\n",
    "        # Final classifier layer\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.net(x)  # Use the pre-trained encoder\n",
    "        x = self.act_fn(self.fc1(x))  # First linear layer with activation\n",
    "        x = self.fc2(x)  # ResNet block to 512\n",
    "        x = self.fc3(x)  # ResNet block to 256\n",
    "        x = self.classifier(x)  # Final classifier layer\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b7abc00-53a3-47d0-811b-ea50dfce45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "encoder_classifier = EncoderWithClassifier(pretrained_encoder=encoder, num_classes=100)\n",
    "encoder_classifier = encoder_classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352ae24-687f-481e-99ae-4d3d166a41db",
   "metadata": {},
   "source": [
    "### Train Encoder with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f724da8-544e-458c-aeeb-d5f54579be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(encoder_classifier.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "01b677dc-7df9-42c8-827c-f186c8c72d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:20\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Users/Sergiu/Desktop/DLComp/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Sergiu/Desktop/DLComp/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    encoder_classifier.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = encoder_classifier(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    if epoch % 4 == 0 and epoch != 0:\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': encoder_classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f'encoder_clf_checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8f5d010-952e-4207-8de5-49e29cdd98d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_classifier.load_state_dict(torch.load('encoder_clf_checkpoint_epoch_6.pth')['model_state_dict']) # This also ruins it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1c31391-94ee-4181-936a-0b3852de4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_clf_20e_emb_2048_train.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca30d7b-f89f-40ff-b106-792f63aebb6a",
   "metadata": {},
   "source": [
    "### Validation of Encoder preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f692598-d5ff-464d-8766-ef9b731a0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "encoder_classifier.eval()\n",
    "\n",
    "# Store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = encoder_classifier(inputs)\n",
    "\n",
    "        # Get the predicted classes (logits to predicted class index)\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and true labels for later analysis\n",
    "        all_predictions.extend(predicted_classes.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for further analysis if needed\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_true_labels = np.array(all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ede9dc9d-2dbd-4ebc-8d5f-a9c564dedba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3510\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics, e.g., accuracy\n",
    "accuracy = np.mean(all_predictions == all_true_labels)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8ee2091-bad6-4785-92bb-ccf849ca46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 702\n",
      "Number of Incorrect Predictions: 1298\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "# Summing the diagonal elements gives the total number of correct predictions\n",
    "correct_predictions = np.trace(cm)\n",
    "total_predictions = cm.sum()\n",
    "\n",
    "print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Number of Incorrect Predictions: {total_predictions - correct_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915e471-660c-44fe-aacb-63f9462140a6",
   "metadata": {},
   "source": [
    "### Train on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485be17-2c33-4b0e-b74a-b3cfbef5ef93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad9334-921d-4dc4-aead-32098539c123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00eb935c-ed8f-429a-9e30-5a1d357af458",
   "metadata": {},
   "source": [
    "### Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9828ae4d-d5a5-45f7-a4bc-a263e99b6f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25546174-f5f7-47f9-a0c3-c65b03e0d935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e088cd3f-d578-4308-9675-ecae6cc0e747",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff79133-5cd5-4bfe-981c-6b66522cef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
