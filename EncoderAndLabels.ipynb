{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678324be-02a3-407b-8bb9-a5acf05b9e02",
   "metadata": {},
   "source": [
    "## Experiment Tracking\n",
    "\n",
    "- Using T8 as Encoder\n",
    "    - Not pre-trained -> accuracy 4%\n",
    "    - pre-trained -> accuracy 64%\n",
    "- Conv Encoder v1 -> Eliminated\n",
    "    - catboost classifier 27% accuracy, 2048 embedding space, 30 epochs\n",
    "    - catboost classifier 27% accuracy, 2048 embedding space, 60 epochs\n",
    "    - encoder classifier 14% accuracy, 2048 embedding space, 60 epochs\n",
    "- Conv Encoder v2\n",
    "    - 768 embedding space, 30 epochs, catboost classifier 53% acc, 1000 iterations\n",
    "    - 768 embedding space, 60 epochs, catboost classifier 55% acc, 3000 iterations\n",
    "    - 1500 embedding space, 30 epochs, catboost classifier 51% acc, 1000 iterations\n",
    "    - 512 embedding space, 30 epochs, catboost classifier 53.7% acc, 1000 iterations, lr=0.01\n",
    "    - 512 embedding space, 60 epochs, catboost classifier 55.6%, 3000 iterations. lr=0.01\n",
    "- Conv Encoder v3\n",
    "    - 1024 embedding space, 30 epochs, catboost classifier 48% acc, 3000 iterations\n",
    "    - 768 embedding space, 30 epochs, catboost classifier 51% acc, 1000 iterations\n",
    "    - 512 embedding space, 30 epochs, catboost classifier ? acc,\n",
    "- VitEncoder\n",
    "    - 768 embedding space, 30 epochs, lr=0.01, catboost 22% acc, 1000 iterations\n",
    "    - 768 embedding space, 70 epochs, lr=0.001 (with adamw),\n",
    "- Conv Encoder v4\n",
    "    - 768 embedding space, 40 epochs, lr=0.01, catboost 54% acc, 2000 iterations\n",
    "    - 768 embedding space, 70 epochs, lr=0.01, catboost 58% acc, 3000 iterations ----------------- 39% on KAggle\n",
    "    - 1024 embedding space, 300 epochs, lr=0.01, catboost 48% acc, 3000 iterations --------------- 30% on KAggle\n",
    "- resnet 18 (pretrained=False) with new fc -> horror\n",
    "    - 100 embedding size, 2%, final fc\n",
    "    - 1000 embedding size, final fc\n",
    "- resnet embedding -> horror\n",
    "- SimpleCNN v2\n",
    "    - 1200 embedding size, 40 epochs, lr = 0.001, bs=128, catboost 42% acc, 3000 iterations\n",
    "- Simple CNN v1\n",
    "    - 1024 embedding size, 100 epochs, lr=0.01, catboost 4% , 3000 iterations, -> horror\n",
    "- Conv Encoder v5\n",
    "    - 4096 embedding size, 26 epochs max, 26% catboost accuracy \n",
    "\n",
    "- Conv v4 + Deep Encoder (from Autoencoder) -> on the catboost trained on train, inserted validation for params, but did not also train on valid\n",
    "    - Conv v4 (768) + deep enc v1 (2048) -> 61% catboost accuracy, -------------------- 56% kaggle\n",
    "    - Conv v2 (768, 60e) + Conv v4 (768, 70e) + deep enc v1 (2048) -> 55%% catboost accuracy, -> v2 seems to drop the performance\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) -> 64.7% accuracy ---------------------------- 61% on kaggle\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) -> 64.4% accuracy (2000 iterations, could probably get out a bit more through more iterations)  -------------\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) + Deep Enc CLF (train + valid) -> 97% accuracy------------- 50% Kaggle WTFFFFF\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) + Deep Enc CLF (train) -> 50 % accuracy------------- no point in uploading to Kaggle\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) + Novel Encoder (train) -> 63% accuracy -> 59% kaggle\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) + mobilenetv3 small (train) -> 62.90% (1000 iter)\n",
    "    - Conv v3 (768, 70e) + Conv v4 (768, 70e) + Deep Encoder V1(2048) + SimpleCNN v2 (1200, 40 epochs) + efficientnet b3 (train) -> 63% (2000 iter) (b3 pare ca are embeddings total useless, catboost nu invata nimic de pe urma lor)\n",
    "- Ablation Studies\n",
    "    - Conv v3 + conv v4 + Simple CNN v2 -> 62.8% catboost (1000 iter + eval)\n",
    "    - deep encoder v1, simple cnn v2 -> 38.9 catboost (1000 iter + eval) -> pare ca deep encoder v1 strica tot, nu poarta aparent nicio valoare\n",
    "    - mobilenet v3 large -> catboost (1000 iter + eval)\n",
    "    - simplecnn v2 + conv v3 + conv v4-> catboost ? (1000 iter + eval)\n",
    "    - simplecnn v2 + conv v3 + convv4 + mobilenet v3 large -> catboost 61.25% (2000 iter + eval) -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5cb06e-e1d1-438d-81d2-3d7a3d9d9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b80202-476f-4bf7-8e5e-ef53aa802ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_classifiers import ConvEncoderV3, ConvEncoderV4, ConvEncoderV5, SimpleCNNV2\n",
    "from utils import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c61576-3156-4018-856a-b73aa6312f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe041c10-7bfb-4612-8127-1fcff76e40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    # img = std * img + mean  # unnormalize\n",
    "    # img = np.clip(img, 0, 1)  # clip any values outside the range [0, 1]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ca793-1709-4bd7-baf7-084a35a72fa2",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3d5656-bf0e-4ac0-b9bc-6eec7c4c03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13000, 2); Valid: (2000, 2); Test: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "valid_df = pd.read_csv('dataset/val.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "print(f\"Train: {train_df.shape}; Valid: {valid_df.shape}; Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e80480-1500-4fa6-afa2-6316f24cbae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Image', 'Class'], dtype='object'),\n",
       "                                       Image  Class\n",
       " 0  0be195e0-eb16-4f29-ac7c-196dec9da47d.png     79\n",
       " 1  28045419-b3b2-415b-9085-b4d241944235.png     94\n",
       " 2  b7078f35-d239-4dd6-babb-1af7be1b9364.png     79\n",
       " 3  0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png     23\n",
       " 4  ba11dda2-37d7-4d28-8bbb-128d452a171c.png     88)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96812ec2-11c9-4ac6-b470-ed90cbe8b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train_images/'\n",
    "val_path = 'dataset/val_images/'\n",
    "test_path = 'dataset/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d696d27-08ec-4247-85b2-886543db9cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png\n"
     ]
    }
   ],
   "source": [
    "train_image_path = os.path.join(train_path,train_df['Image'][0])\n",
    "print(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63e66e4-62b1-458b-a8ca-b8c735d139cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e1b66ed-69a3-4899-ba3f-2b8008d42631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71603f8-3b99-43d4-9b18-5b97ea67c617",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d76956ba-4bac-4e9f-8168-6ae203081158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png', 'dataset/train_images/28045419-b3b2-415b-9085-b4d241944235.png', 'dataset/train_images/b7078f35-d239-4dd6-babb-1af7be1b9364.png', 'dataset/train_images/0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png', 'dataset/train_images/ba11dda2-37d7-4d28-8bbb-128d452a171c.png'] [79, 94, 79, 23, 88]\n"
     ]
    }
   ],
   "source": [
    "train_image_names = train_df['Image'].tolist()\n",
    "train_image_labels = train_df['Class'].tolist()\n",
    "# print(train_image_names[:5], train_image_labels[:5])\n",
    "\n",
    "train_image_paths = [os.path.join(train_path, image_name)for image_name in train_image_names]\n",
    "print(train_image_paths[:5], train_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5cb30a3-4d79-4824-8328-970b7faebc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/val_images/e91a8fbc-d3ba-4b39-8c2f-04c14de78e5e.png', 'dataset/val_images/7c40819b-c3ce-4a91-9e98-c3df11b63623.png', 'dataset/val_images/d54269d7-fe86-4112-9c0f-99cc6ab8d9c0.png', 'dataset/val_images/cbf9ac9e-0859-4b54-ae65-347587b45deb.png', 'dataset/val_images/6aafce3f-9002-44e0-9a99-ffe9b49c9bac.png'] [32, 85, 41, 97, 62]\n"
     ]
    }
   ],
   "source": [
    "val_image_names = valid_df['Image'].tolist()\n",
    "val_image_labels = valid_df['Class'].tolist()\n",
    "\n",
    "val_image_paths = [os.path.join(val_path, image_name)for image_name in val_image_names]\n",
    "print(val_image_paths[:5], val_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c67bd7-9ce2-499c-b6c1-d8407b84e0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/test_images/046f61c4-b825-459a-8b2d-07503f5b94a5.png', 'dataset/test_images/67db001f-e287-4950-ac49-6683b493d1a4.png', 'dataset/test_images/9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png', 'dataset/test_images/5ffef91a-aaf9-4d0d-a219-83a9f5282361.png', 'dataset/test_images/c00af570-0000-4f8f-a3f2-c37a981bfdb1.png']\n"
     ]
    }
   ],
   "source": [
    "test_image_names = test_df['Image'].tolist()\n",
    "test_image_paths = [os.path.join(test_path, image_name)for image_name in test_image_names]\n",
    "print(test_image_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a5e8b-dc19-4682-a3a4-34874dd7b2ad",
   "metadata": {},
   "source": [
    "### Load Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c94186b-7927-4106-aa72-c88b7b4d3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "encoder = ConvEncoderV3(embedding_dim=768)\n",
    "encoder.load_state_dict(torch.load('conv_encoder_v3_768_30e_train.pth'))\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339805c-2224-449d-9817-4dc9b1a5cf11",
   "metadata": {},
   "source": [
    "#### Conv Encoder v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00507d04-15f3-4136-8db0-982c6230aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = ConvEncoderV4(embedding_dim=768)\n",
    "encoder.load_state_dict(torch.load('conv_enc_v4_768_emb_70e_train.pth'))\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8aac9-9b69-414b-ad15-d2b3f1591ece",
   "metadata": {},
   "source": [
    "#### Simple CNN v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9246a353-fe79-462b-a0fb-023498ebf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SimpleCNNV2(embedding_dim=1200)\n",
    "encoder.load_state_dict(torch.load('simple_cnn_v2_40e_emb_1200_train.pth'))\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627155ac-14c8-4c3b-a912-02f424e9b112",
   "metadata": {},
   "source": [
    "#### Conv Encoder v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc284fc-ffbd-44fa-b66a-97077ef96c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "encoder = ConvEncoderV5(embedding_dim = 4096)\n",
    "encoder.load_state_dict(torch.load('conv_encoder_v5_checkpoint_epoch_21.pth')['encoder_state_dict'])\n",
    "# input_image = torch.randn(1, 3, 64, 64) # Example input tensor (batch_size, channels, height, width)\n",
    "# output = encoder(input_image)\n",
    "# print(output.shape)  # Output shape after passing through the encoder\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cb441-41b5-4804-b466-f7f0692e0031",
   "metadata": {},
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "178f4b48-11e7-41c0-9ff1-40cc7c732049",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transform = T.Compose([\n",
    "    # T.Resize((64, 64)), # -> all are already 64 * 64\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.42835271, 0.40658227, 0.34071648], [0.2144312,  0.21884131, 0.20464434])\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d7a78e5-45d7-4a2c-9f3f-55d7d4110db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process an image and get embedding\n",
    "def get_embedding(image_path, encoder, flatten=True):\n",
    "    image = Image.open(image_path)\n",
    "    image = basic_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = encoder(image)\n",
    "    \n",
    "    if flatten:\n",
    "        # Flatten the embedding\n",
    "        embedding = torch.flatten(embedding, start_dim=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25c6e3f0-6fcb-4ac1-8676-3f0a2d9c562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embedding(os.path.join(train_path, train_image_names[0]), encoder, flatten=True)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f882f3-f067-4c42-814a-9ec463eaddf8",
   "metadata": {},
   "source": [
    "### Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b94ef15-9566-4ea5-98ef-3584845da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    # T.RandomHorizontalFlip(p=0.5),\n",
    "    # T.RandomVerticalFlip(p=0.5),\n",
    "    # T.Resize((64, 64)), -> all are already 64 * 64\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f5a753-343f-4d62-9244-8f099590377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_image_paths, train_image_labels, transform = train_transform)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f68f7-0c20-4c41-8642-fbd3305a4ab9",
   "metadata": {},
   "source": [
    "### Get Mean and Std for DataLoader, and create Transform with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "3201a8e3-4819-4cf7-949b-c2a8504f123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.42835271 0.40658227 0.34071648]\n",
      "Std: [0.2144312  0.21884131 0.20464434]\n"
     ]
    }
   ],
   "source": [
    "# def compute_mean_std(directory):\n",
    "#     means, stds = [], []\n",
    "#     for img_filename in os.listdir(directory):\n",
    "#         img_path = os.path.join(directory, img_filename)\n",
    "#         with Image.open(img_path) as img:\n",
    "#             img_arr = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "#             means.append(np.mean(img_arr, axis=(0, 1)))  # Mean across height and width\n",
    "#             stds.append(np.std(img_arr, axis=(0, 1)))    # Std across height and width\n",
    "\n",
    "#     mean = np.mean(means, axis=0)  # Mean across all images\n",
    "#     std = np.mean(stds, axis=0)    # Std across all images\n",
    "\n",
    "#     return mean, std\n",
    "\n",
    "# mean, std = compute_mean_std(train_path)\n",
    "# print(f\"Mean: {mean}\")\n",
    "# print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ecbcdda-fe9e-4806-81ab-9cb99835a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c5fd6-f557-4f27-86da-c0a2c882ed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 6.9631\n",
      "Epoch [2/50], Loss: 6.9633\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = []\n",
    "num_epochs = 50 \n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings = encoder(images)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': losses\n",
    "        }\n",
    "        torch.save(checkpoint, f'vit_enc_v2_checkpoint_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84d3c61-6cf7-4f89-a8e5-728972a020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save encoder\n",
    "torch.save(encoder.state_dict(), 'vit_encoder_v2_50e_emb_1024.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0378ad7-3b37-487c-b3fc-9d5d0d1e98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(f'conv_encoder_v5_checkpoint_epoch_21.pth')['encoder_state_dict'])\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db438a2f-a168-46b3-926f-df2a6079f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load for inference\n",
    "# encoder = ImageEncoder(embedding_dim=1024)\n",
    "# encoder.load_state_dict(torch.load('conv_enc_v4_300e_bs_128_emb_1024_train.pth'))\n",
    "# encoder = encoder.to(device)\n",
    "# encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e084d07-30e2-4ae6-8ede-7f1edfd2c015",
   "metadata": {},
   "source": [
    "### Training Data for CatBoost\n",
    "Here we will load the embeddings generated by vision models, in an attempt at classifying images based on the embeddings generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ee9875-8153-46e7-8031-48fc307da227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [get_embedding(img_path, encoder).cpu() for img_path in train_image_paths]\n",
    "val_embeddings = [get_embedding(img_path, encoder).cpu() for img_path in val_image_paths]\n",
    "test_embeddings = [get_embedding(img_path, encoder).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69cfc87a-5f8e-492a-ab6c-b3bf5f5abf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Load data (deserialize)\n",
    "# with open('train_embeddings_mobilenet_v3_large.pkl', 'rb') as handle:\n",
    "#     train_embeddings_ce = pickle.load(handle)\n",
    "\n",
    "# # Load data (deserialize)\n",
    "# with open('val_embeddings_mobilenet_v3_large.pkl', 'rb') as handle:\n",
    "#     val_embeddings_ce = pickle.load(handle)\n",
    "\n",
    "# # Load data (deserialize)\n",
    "# with open('test_embeddings_mobilenet_v3_large.pkl', 'rb') as handle:\n",
    "#     test_embeddings_ce = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfe36b2b-f9a3-4ab9-ae0f-418fb25b25c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embeddings_ce[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52477072-4c24-4631-8d04-430e3aa67e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load data (deserialize)\n",
    "with open('train_embeddings_simple_cnn_v2.pkl', 'rb') as handle:\n",
    "    train_embeddings_v2 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('val_embeddings_simple_cnn_v2.pkl', 'rb') as handle:\n",
    "    val_embeddings_v2 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('test_embeddings_simple_cnn_v2.pkl', 'rb') as handle:\n",
    "    test_embeddings_v2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e72be3c-9128-47b0-be98-bbd12edde16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load data (deserialize)\n",
    "with open('train_embeddings_conv_enc_v3.pkl', 'rb') as handle:\n",
    "    train_embeddings_v3 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('val_embeddings_conv_enc_v3.pkl', 'rb') as handle:\n",
    "    val_embeddings_v3 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('test_embeddings_conv_enc_v3.pkl', 'rb') as handle:\n",
    "    test_embeddings_v3 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ae1543d-364d-452a-a852-fcc1949d69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load data (deserialize)\n",
    "with open('train_embeddings_conv_enc_v4.pkl', 'rb') as handle:\n",
    "    train_embeddings_v4 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('val_embeddings_conv_enc_v4.pkl', 'rb') as handle:\n",
    "    val_embeddings_v4 = pickle.load(handle)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('test_embeddings_conv_enc_v4.pkl', 'rb') as handle:\n",
    "    test_embeddings_v4 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "478b398b-0a84-482a-8582-cf3beac1a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_all = []\n",
    "for train_embedding_v2, train_embedding_v3, train_embedding_v4 in zip(train_embeddings_v2, train_embeddings_v3, train_embeddings_v4):\n",
    "    concatenated_tensor = torch.cat((train_embedding_v2, train_embedding_v3, train_embedding_v4), dim=0)\n",
    "    train_embeddings_all.append(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55b9016e-513c-42eb-a200-389a4766f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_all = []\n",
    "for val_embedding_v2, val_embedding_v3, val_embedding_v4 in zip(val_embeddings_v2, val_embeddings_v3, val_embeddings_v4):\n",
    "    concatenated_tensor = torch.cat((val_embedding_v2, val_embedding_v3, val_embedding_v4), dim=0)\n",
    "    val_embeddings_all.append(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71928da4-695e-4ad8-91d5-275d654916d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 4784\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embeddings_all), len(train_embeddings_all[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2e98151-8477-49a9-b82d-4311e1ae15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 4784\n"
     ]
    }
   ],
   "source": [
    "print(len(val_embeddings_all), len(val_embeddings_all[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79056ed-2b98-480d-941e-e1789c5534c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 2048\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embeddings_ce), len(train_embeddings_ce[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e50120f0-d123-4b91-9ff5-4d22b7155d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 1200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embeddings_v2), len(train_embeddings_v2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2d51af-98ba-4b34-a201-df6de5ddbabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 768\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embeddings_v3), len(train_embeddings_v3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be44188-0590-4a0f-81a0-7a099175dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 768\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embeddings_v4), len(train_embeddings_v4[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f29e00de-013d-49da-b8ca-4ab0d6fb0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example embeddings and labels\n",
    "X = np.array(train_embeddings_all)  # image_paths is a list of image file paths\n",
    "y = np.array(train_image_labels)  # labels for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e3cb9cb-039f-499d-a1ef-6d06391ef2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(val_embeddings_all)  # image_paths is a list of image file paths\n",
    "y_val = np.array(val_image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b1231-76da-4d2f-a951-f6c911788915",
   "metadata": {},
   "source": [
    "### Train Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ae88ad-609d-460f-a427-49e0c4100b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CatBoost Classifier\n",
    "catboost_model = CatBoostClassifier(use_best_model=True, loss_function=\"MultiClass\", task_type=\"GPU\", iterations=2000)\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X, y, eval_set=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc22fa8-a9d1-4e58-8100-05d0c0e0ce5e",
   "metadata": {},
   "source": [
    "#### Compute Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbe7736b-1f4c-44b9-b4a3-e378b3c465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = catboost_model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32b2331d-818a-4f8f-9081-fc9c0a44144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(importances, index=[i for i in range(6832)]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "123073d7-e88d-460e-a870-3db8b9b40f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4112    2.149597\n",
       "4115    1.798838\n",
       "4822    1.779284\n",
       "3347    1.548376\n",
       "4036    1.350348\n",
       "          ...   \n",
       "4016    0.295921\n",
       "4097    0.291356\n",
       "3330    0.290661\n",
       "4111    0.282728\n",
       "4045    0.276977\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dcd77-85b5-454d-8a1a-a57294a53119",
   "metadata": {},
   "source": [
    "### Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea67cc3-e9d7-4366-b914-35dc806acdaa",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7fb04d3-efdf-41e5-b434-b52cd8b6891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_labels = catboost_model.predict(val_embeddings_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932a608-3c34-4881-b2ed-46b8536d5c0f",
   "metadata": {},
   "source": [
    "#### Encoder Preds - usually worse than taking the embedding space and using a clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4c27705f-4e4b-4b55-94bf-4b85dec68690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = CustomDataset(val_image_paths, val_image_labels)\n",
    "# val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "44529672-8b3d-470b-81d1-58f23cf35a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.6717, 0.5947, 0.6015])\n",
      "Std: tensor([0.2283, 0.2231, 0.2238])\n"
     ]
    }
   ],
   "source": [
    "# mean, std = get_mean_std(val_data_loader)\n",
    "# print(f\"Mean: {mean}\")\n",
    "# print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0d194dcc-fd29-4dbf-8212-ea9aaa896db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_val = T.Compose([\n",
    "#     # T.RandomHorizontalFlip(p=0.5),\n",
    "#     # T.RandomVerticalFlip(p=0.5),\n",
    "#     # T.Resize((64, 64)), -> all are already 64 * 64\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize([0.6717, 0.5947, 0.6015], [0.2283, 0.2231, 0.2238])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "34ba3e15-8c8e-4a59-85da-f638ce3dae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6422\n",
      "CPU times: user 4.68 s, sys: 623 ms, total: 5.3 s\n",
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# val_pred_labels = []\n",
    "# for images, _ in val_data_loader:\n",
    "#     images = images.to(device)\n",
    "#     # labels = labels.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         logits = encoder(images)\n",
    "#         predicted_class = torch.argmax(logits, dim=1)  # Get the index of the max logit\n",
    "#         val_pred_labels.append(predicted_class.item())  # Return the class index as an integer\n",
    "\n",
    "#     # optimizer.zero_grad()\n",
    "#     # loss.backward()\n",
    "#     # optimizer.step()\n",
    "\n",
    "# print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "5c7fe26b-5a6e-452d-ae12-373bd9e426cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_image(image_path):\n",
    "#     image = Image.open(image_path)\n",
    "#     return transform_val(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# def predict_class(image_path, model):\n",
    "#     image_tensor = prepare_image(image_path)\n",
    "#     image_tensor = image_tensor.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(image_tensor)\n",
    "#         predicted_class = torch.argmax(logits, dim=1)  # Get the index of the max logit\n",
    "#         return predicted_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a72e2e18-148a-45eb-a55c-2a78962d36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred_labels = [predict_class(path, encoder) for path in val_image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e23e91-5ccd-4ad7-a357-cc5d2407ac56",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26e9451d-4b50-4955-beb8-ff33e98d8847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 63.05%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(val_image_labels, val_pred_labels)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1399fd2c-47f6-4169-bb96-14fbfe9eee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Accuracy: 1.00%\n"
     ]
    }
   ],
   "source": [
    "# def top_n_accuracy(true_labels, pred_probs, n=3):\n",
    "#     best_n = np.argsort(pred_probs, axis=1)[:,-n:]\n",
    "#     successes = 0\n",
    "#     for i in range(pred_probs.shape[0]):\n",
    "#         if true_labels[i] in best_n[i,:]:\n",
    "#             successes += 1\n",
    "#     return successes / pred_probs.shape[0]\n",
    "\n",
    "# # Example usage\n",
    "# top3_acc = top_n_accuracy(val_image_labels, val_pred_labels, n=3)\n",
    "# print(f\"Top 3 Accuracy: {top3_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3127f-d4ef-4780-b2ba-75e1fc0ff9be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2c2c8853-9d73-41c4-851f-a4d886d5537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = classification_report(val_image_labels, val_pred_labels)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6416a0-0d6e-41aa-8576-ac18c10daf4f",
   "metadata": {},
   "source": [
    "#### CM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ce1cb5d-84bc-4399-8c9f-4deed9598a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 1012\n",
      "Number of Incorrect Predictions: 988\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(val_image_labels, val_pred_labels)\n",
    "\n",
    "# Summing the diagonal elements gives the total number of correct predictions\n",
    "correct_predictions = np.trace(cm)\n",
    "total_predictions = cm.sum()\n",
    "\n",
    "print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Number of Incorrect Predictions: {total_predictions - correct_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7fe43-6224-4966-85e8-62bdc0a30039",
   "metadata": {},
   "source": [
    "### Train Encoder on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2800e1fa-b588-408a-b608-b194c091c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(val_image_paths, val_image_labels, transform=basic_transform)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d370679-45d9-4cf9-b1a0-ed8d94ba8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6e56c9-4f1b-410b-aac0-7005408f7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_epochs = 100 \n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        embeddings = encoder(images)\n",
    "        loss = criterion(embeddings, labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba70a5-788a-42ce-8b57-102d19b2a606",
   "metadata": {},
   "source": [
    "### Train Classification Model on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bfbc21d-0228-4bf0-b6fa-044d1736d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings = [get_embedding(img_path).cpu() for img_path in val_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8acb5ec9-7d3a-4387-a325-4c26a83c00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(val_embeddings)\n",
    "y_val = np.array(val_image_labels)\n",
    "\n",
    "catboost_model = CatBoostClassifier(loss_function=\"MultiClass\", task_type=\"GPU\", iterations=3000)\n",
    "\n",
    "catboost_model.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d73cb-a6fa-47dc-99ee-55f5234a407f",
   "metadata": {},
   "source": [
    "### Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "008b0be1-8de5-4e58-847e-cfa3b026894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = [get_embedding(img_path).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32555394-e896-407e-9ea1-8d7f3257656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/test_images/046f61c4-b825-459a-8b2d-07503f5b94a5.png', 'dataset/test_images/67db001f-e287-4950-ac49-6683b493d1a4.png', 'dataset/test_images/9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png', 'dataset/test_images/5ffef91a-aaf9-4d0d-a219-83a9f5282361.png', 'dataset/test_images/c00af570-0000-4f8f-a3f2-c37a981bfdb1.png']\n",
      "\n",
      "0    046f61c4-b825-459a-8b2d-07503f5b94a5.png\n",
      "1    67db001f-e287-4950-ac49-6683b493d1a4.png\n",
      "2    9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png\n",
      "3    5ffef91a-aaf9-4d0d-a219-83a9f5282361.png\n",
      "4    c00af570-0000-4f8f-a3f2-c37a981bfdb1.png\n",
      "Name: Image, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# test the order\n",
    "print(test_image_paths[:5])\n",
    "print()\n",
    "print(test_df['Image'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea1ad9d7-2cb1-4ac9-ba1c-3b114cc2ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 6784\n"
     ]
    }
   ],
   "source": [
    "print(len(test_embeddings_all), len(test_embeddings_all[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6e4dfe8-bd00-4b8b-9143-fd0369c1a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a4bc058-414f-46bc-a369-6f3e4746248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = catboost_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "572f0928-fd6c-4b8d-8390-86ffa514e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Class'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1739c50-3402-4cea-bb15-36c0a426a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046f61c4-b825-459a-8b2d-07503f5b94a5.png</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67db001f-e287-4950-ac49-6683b493d1a4.png</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ffef91a-aaf9-4d0d-a219-83a9f5282361.png</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c00af570-0000-4f8f-a3f2-c37a981bfdb1.png</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Image  Class\n",
       "0  046f61c4-b825-459a-8b2d-07503f5b94a5.png     67\n",
       "1  67db001f-e287-4950-ac49-6683b493d1a4.png     82\n",
       "2  9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png     40\n",
       "3  5ffef91a-aaf9-4d0d-a219-83a9f5282361.png     13\n",
       "4  c00af570-0000-4f8f-a3f2-c37a981bfdb1.png     69"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513e62f-f5fe-430d-b83d-453e0d0d6c4c",
   "metadata": {},
   "source": [
    "### Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2935ef16-31c6-4089-a5e0-2bc555bb243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission_conv_simple_cnn_v2_conv_enc_v3_v4_and_deep_enc_v1_nov_enc_cb_no_train_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3b77e-5705-4770-b6c3-9478d0cfb676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
