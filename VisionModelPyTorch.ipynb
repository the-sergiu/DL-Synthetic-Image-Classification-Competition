{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e1c952-38c1-4016-9acb-a786314570a3",
   "metadata": {},
   "source": [
    "#### Mobilenet\n",
    "- MobileNetV3Small (embedding generation using Cross Entropy Loss, 2048) -> validation accuracy is 60% after 2 rounds of  -> `mobilenet_v3_ft_epoch_6.pth`\n",
    "- MobileNetV3Large (embedding generation using Cross Entropy Loss, 2048) -> validation accuracy is 65% -> after 1 round of ft -> `mobilenet_large_v3_ft_epoch_5.pth`\n",
    "\n",
    "#### EfficientNet\n",
    "- EfficientNet B3 (embedding generation using CE, 2048) -> validation accuracy is 67% -> after 2 rounds of ft -> `efficientnet_b3_ftft_epoch_3` ----Kaggle 64%\n",
    "- EfficientNet V2 M (embedding generation using CE, 2048) -> validation accuracy is 64% -> after 2 rounds of ft -> `efficientnet_v2_m_ftft_epoch_3`\n",
    "- EfficientNet B4 (embedding generation using CE, 2048) -> validation accuracy is 64.7% -> after 2 rounds of ft -> ``\n",
    "- EfficientNet B4 with Aug (embedding generation using CE, 2048) -> validation accuracy is 57.7% -> after 2 rounds of ft\n",
    "- EfficientNet B3 With AUG (embedding generation using CE, 4096) -> validation accuracy is 65% -> after 1 round of ft (second round didn't change anything) -> `efficientnet_b3_aug_ft_epoch_3`\n",
    "\n",
    "#### Others\n",
    "- Max Vit T (without AUG) -> 45-50% after ft\n",
    "- Deep Encoder 52% after ft (WITH AUG)\n",
    "- Convnext Tiny (AUG) 1000 embedding and  -> 52%\n",
    "- Small(no AUG) 2048 embedding -> 50 (Convnexts converge very fast, but get stuck there)\n",
    "- **ConvV4 (no AUG) 1024 embedding -> 78.6% -> `convv4_ft_5` -> 75% Kaggle**\n",
    "- **ConvV3 (no AUG) 1024 embedding -> 78.4% (lost the 79.5) -> `convv3_ft_3` -> ?% Kaggle**\n",
    "- Conv V5 2048 -> 62%\n",
    "- **Conv V6 1024 -> 79.3% -> `convv6_ftft_epoch_3`**\n",
    "\n",
    "#### Ensemble\n",
    "- MobileNetv3 large, EfficientNet B3, EfficientNet b4 -> 70.9% accuracy on validation, 68.2% Kaggle\n",
    "- MobileNetv3 large, EfficientNet B3, EfficientNet b4, efficient Net v2M -> 71.1 accuracy on validation, ? on Kaggle\n",
    "- MobileNetv3 samll, MobileNetv3 large, EfficientNet V3, EfficientNet b4, efficient Net v2M -> 70.1%, mobilenet v3 small decreases efficiency\n",
    "- MobileNetv3 large, EfficientNet B3, Efficient net B3 with Aug, EfficientNet b4, efficient Net v2M -> 73.7 accuracy on validation, 72 on Kaggle\n",
    "- MobileNetv3 large, EfficientNet B3, Efficient net B3 with Aug, EfficientNet b4, efficient Net v2M, convv4 -> 79.15 accuracy on validation -> Kaggle ?\n",
    "- MobileNetv3 large (65), EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4 (65), convv4 (79) -> 76.45 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), convv4 (70) -> 78.75 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4 (65), convv4 (79) -> 79.55 accuracy on validation **this is probably better than our current submission**\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M , convv4 (79) -> 79.3 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M (65), convv3, convv4 (79) -> 81.15% accuracy on validation (80.25 now)\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4  (65), convv3, convv4 (79) -> 80.5% accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), convv3, convv4 (79) -> 79.9 accuracy on validation\n",
    "- EfficientNet B3 (67), convv3, convv4 (79) -> 80.85% accuracy on validation\n",
    "- Efficient net B3 with Aug (65), convv3, convv4 (79) -> 80.85%\n",
    "- convv3, convv4 -> 79.55%\n",
    "- **EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M  (65), convv3, convv4 (78), conv6 (79) -> 81.5% accuracy on validation**\n",
    "- convv3, convv4 (78), conv6 (79) -> 81.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe5f047-2d7f-4bc7-aac3-12aa78ec9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baadf362-e7cc-4b2f-8a96-45863a4d2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_classifiers import ConvEncoderV3, ConvEncoderV4, ConvEncoderV5, ConvEncoderV6\n",
    "from pytorch_classifiers import (\n",
    "    EfficientNetB3,\n",
    "    EfficientNetB4,\n",
    "    ConvNextT, \n",
    "    MobileNetV3Small, \n",
    "    MobileNetV3Large, \n",
    "    SwinV2TEncoder,\n",
    "    EfficientNetV2M,\n",
    "    ensemble_majority_voting\n",
    ")\n",
    "from utils import imshow, CustomDataset, TestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6f4c1a-018e-468e-8d9b-e26ab861994f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501b852-8c7c-44ef-a133-90203b3133b3",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9175c9-721d-470d-95cc-e27c461fb59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13000, 2); Valid: (2000, 2); Test: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "valid_df = pd.read_csv('dataset/val.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "print(f\"Train: {train_df.shape}; Valid: {valid_df.shape}; Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f017f50-261f-4c77-a49f-10a1919cbfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Image', 'Class'], dtype='object'),\n",
       "                                       Image  Class\n",
       " 0  0be195e0-eb16-4f29-ac7c-196dec9da47d.png     79\n",
       " 1  28045419-b3b2-415b-9085-b4d241944235.png     94\n",
       " 2  b7078f35-d239-4dd6-babb-1af7be1b9364.png     79\n",
       " 3  0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png     23\n",
       " 4  ba11dda2-37d7-4d28-8bbb-128d452a171c.png     88)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885a3312-cadf-4dd4-ad7c-36e3349d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train_images/'\n",
    "val_path = 'dataset/val_images/'\n",
    "test_path = 'dataset/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e46d38-1445-4c3e-8ccd-3eff5d8a3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png\n"
     ]
    }
   ],
   "source": [
    "train_image_path = os.path.join(train_path,train_df['Image'][0])\n",
    "print(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0467d5b0-4ac5-46e7-af4e-094d47b984e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf480f43-4f41-426b-acea-59550d127954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/train_images/0be195e0-eb16-4f29-ac7c-196dec9da47d.png', 'dataset/train_images/28045419-b3b2-415b-9085-b4d241944235.png', 'dataset/train_images/b7078f35-d239-4dd6-babb-1af7be1b9364.png', 'dataset/train_images/0f54f663-2953-432b-bdd4-9b9f7a78bfb9.png', 'dataset/train_images/ba11dda2-37d7-4d28-8bbb-128d452a171c.png'] [79, 94, 79, 23, 88]\n"
     ]
    }
   ],
   "source": [
    "train_image_names = train_df['Image'].tolist()\n",
    "train_image_labels = train_df['Class'].tolist()\n",
    "# print(train_image_names[:5], train_image_labels[:5])\n",
    "\n",
    "train_image_paths = [os.path.join(train_path, image_name)for image_name in train_image_names]\n",
    "print(train_image_paths[:5], train_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b09de1-bda9-42e9-a87e-fe2fc03025a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/val_images/e91a8fbc-d3ba-4b39-8c2f-04c14de78e5e.png', 'dataset/val_images/7c40819b-c3ce-4a91-9e98-c3df11b63623.png', 'dataset/val_images/d54269d7-fe86-4112-9c0f-99cc6ab8d9c0.png', 'dataset/val_images/cbf9ac9e-0859-4b54-ae65-347587b45deb.png', 'dataset/val_images/6aafce3f-9002-44e0-9a99-ffe9b49c9bac.png'] [32, 85, 41, 97, 62]\n"
     ]
    }
   ],
   "source": [
    "val_image_names = valid_df['Image'].tolist()\n",
    "val_image_labels = valid_df['Class'].tolist()\n",
    "\n",
    "val_image_paths = [os.path.join(val_path, image_name)for image_name in val_image_names]\n",
    "print(val_image_paths[:5], val_image_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1dff425-5139-4e83-8fc0-09611ac28842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/test_images/046f61c4-b825-459a-8b2d-07503f5b94a5.png', 'dataset/test_images/67db001f-e287-4950-ac49-6683b493d1a4.png', 'dataset/test_images/9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png', 'dataset/test_images/5ffef91a-aaf9-4d0d-a219-83a9f5282361.png', 'dataset/test_images/c00af570-0000-4f8f-a3f2-c37a981bfdb1.png']\n"
     ]
    }
   ],
   "source": [
    "test_image_names = test_df['Image'].tolist()\n",
    "test_image_paths = [os.path.join(test_path, image_name)for image_name in test_image_names]\n",
    "print(test_image_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9568be-356d-4b7f-8ba5-0530493f4227",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a33e00e-a3d2-4283-9384-c85daedf5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = T.Compose([\n",
    "    # T.RandomHorizontalFlip(p=0.5),\n",
    "    # T.RandomVerticalFlip(p=0.5),\n",
    "    # T.RandomRotation(30),\n",
    "    # T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.42835271, 0.40658227, 0.34071648], [0.2144312,  0.21884131, 0.20464434])\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e18014c-6927-4a4e-942b-44ff3c29b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = T.Compose([\n",
    "    # transforms.Resize((64, 26424)),  # Resize the image to 224x224 (MobileNetV3 input size)\n",
    "    # T.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    # T.RandomRotation(10),  # Randomly rotate images in the range (-10, 10) degrees\n",
    "    T.ToTensor(),  # Convert PIL image to tensor\n",
    "    T.Normalize((0.5),(0.5)),  # Normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06530fae-8ded-43e9-bd87-54489a72305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_image_paths, train_image_labels, transform = transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609061e3-5dda-4ec0-a3c9-aab9938c2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(val_image_paths, val_image_labels, transform = val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620ad3-ceb6-4b20-9409-b6396cf1ad2e",
   "metadata": {},
   "source": [
    "## Conv V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ae6f70-6ae7-4d12-873b-313b5ac96647",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvEncoderV6(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load('convv6_ftft_epoch_3.pth')['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b521184-ee47-42e0-bd97-06fc8a9282c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# optimizer.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3475d166-1c24-4e41-8eb8-e24d0451afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.2412593969527413\n",
      "Validation Loss: 0.6346, Accuracy: 79.20%\n",
      "Epoch 2/3, Loss: 0.23333924205279818\n",
      "Validation Loss: 0.6312, Accuracy: 78.85%\n",
      "Epoch 3/3, Loss: 0.2306116392799452\n",
      "Validation Loss: 0.6324, Accuracy: 79.30%\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3 # Number of epochs\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'convv6_ftft_epoch_{epoch}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245b079-ca77-419b-9148-a4c1711a2087",
   "metadata": {},
   "source": [
    "## Conv Encoder V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6da9194-967e-46ab-af8e-bd2cb56f12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvEncoderV5(embedding_dim=2048)\n",
    "model.load_state_dict(torch.load('convv5_epoch_6.pth')['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ad7509-e4a6-4bae-8ffc-c5c05e87b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# optimizer.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89f5616-e84f-47b4-9138-a08b2eed1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.029198386505538344\n",
      "Validation Loss: 1.3514, Accuracy: 61.95%\n",
      "Epoch 2/20, Loss: 0.022266855235120245\n",
      "Validation Loss: 1.3406, Accuracy: 62.20%\n",
      "Epoch 3/20, Loss: 0.019844524845407874\n",
      "Validation Loss: 1.3454, Accuracy: 61.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20 # Number of epochs\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'convv5_ft_epoch_{epoch}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffdd81-8cb0-48c7-ac1c-b4cfeb57736d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cffef1-24c6-4f4e-b5ad-f118954802c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ConvEncoder v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8f3c29a-23a1-45f0-aabe-6977c896c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvEncoderV4(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load('convv4_ft_5.pth')['model_state_dict'])\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa18d527-a3a5-4fec-a205-bc07172adc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fea40867-5094-4f55-a88a-799c8c618047",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# optimizer.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1648d90b-a033-46b9-94d4-09f0944a6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 4 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'convv4_ftft_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35eea07-b344-40bf-841b-6a7c87deb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.extend(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2922625-cbc3-48a0-b59c-ce05140eb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 85, 72, 97, 62] 2000\n"
     ]
    }
   ],
   "source": [
    "preds = [pred.item() for pred in preds]\n",
    "print(preds[:5], len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d01df318-457d-43c0-b0da-ac7f8e728283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 43.95%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(val_image_labels, preds)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e7e0d6c-0a8a-4a38-b368-7f1ad811a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 879\n",
      "Number of Incorrect Predictions: 1121\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(val_image_labels, preds)\n",
    "\n",
    "# Summing the diagonal elements gives the total number of correct predictions\n",
    "correct_predictions = np.trace(cm)\n",
    "total_predictions = cm.sum()\n",
    "\n",
    "print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Number of Incorrect Predictions: {total_predictions - correct_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2607f-2443-4f7d-a790-c57155660aa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conv V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0081bca0-4a92-40b2-b482-5cff3148cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvEncoderV3(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load('convv3_ftft_2_78.4.pth')['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be2d6cd9-34fe-46f3-8b00-64a178518757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d81b7ca9-c06b-4806-a059-b46fdcefda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# optimizer.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6143d26d-2fe9-49e4-8a2d-d5497ea572a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.18260784330321292\n",
      "Validation Loss: 1.1287, Accuracy: 73.40%\n",
      "Epoch 2/20, Loss: 0.14187204859712543\n",
      "Validation Loss: 1.0575, Accuracy: 73.95%\n",
      "Epoch 3/20, Loss: 0.11881564225198007\n",
      "Validation Loss: 1.2171, Accuracy: 71.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'convv3_ftft_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4758fd1-789a-407e-b87c-a66560c8c28b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ConvNextT Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9119894-9ea8-4539-b01a-772c3c39f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNextT().to(device)\n",
    "# model.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aca7042-bc7d-4eb9-8a66-5104cc95248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4362724d-e484-4cb6-b184-1bce2206b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer.load_state_dict(torch.load('convnext_tiny_aug_7.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f50510bd-3543-4ba1-a085-0f4f46efe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'convnext_tiny_2048_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09f5664-7f0a-4e73-9ad1-2f5e26fa8c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('convnext_tiny_2048_6.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e194f0e-2765-467f-8efa-307190706f8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MobileNetV3 Small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68a4cb-872a-4d1b-a50d-66419ff7b43c",
   "metadata": {},
   "source": [
    "### Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e58e0433-0cfd-44e7-b4a7-ad4d3b525291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV3Small(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "324d437f-4f4e-4735-b681-bd1ec7b7e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a793e4b-a162-4e62-b878-03e20bf935f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392a471-757b-4ae7-9e7c-09d7cf0dd23d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9837d0aa-d14d-47c0-ae1d-4fdbd84ead8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'mobilenet_v3_ftft_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df0ef299-d38d-4ddc-a3f9-0c390a854d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('mobilenet_v3_epoch_21.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbb12a5d-bae8-4220-bb55-f3b8d1ac76de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mobilenet_v3_ft_epoch_6.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c212fcd-e8b0-4e75-93b9-7cc13acf6939",
   "metadata": {},
   "source": [
    "### Embedding Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c378e1f4-18df-4ed4-a9b8-bdba31278bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transform = T.Compose([\n",
    "    # T.Resize((64, 64)), # -> all are already 64 * 64\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.42835271, 0.40658227, 0.34071648], [0.2144312,  0.21884131, 0.20464434])\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e380e3a-01b4-4f57-bdd3-c335b4f23367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process an image and get embedding\n",
    "def get_embedding(image_path, encoder, flatten=True):\n",
    "    image = Image.open(image_path)\n",
    "    image = basic_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = encoder(image)\n",
    "    \n",
    "    if flatten:\n",
    "        # Flatten the embedding\n",
    "        embedding = torch.flatten(embedding, start_dim=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b849862e-0611-4388-a6b4-e2aea4e174f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embedding(os.path.join(train_path, train_image_names[0]), model, flatten=True)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c632c65-599c-4251-a704-e891522df0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [get_embedding(img_path, model).cpu() for img_path in train_image_paths]\n",
    "val_embeddings = [get_embedding(img_path, model).cpu() for img_path in val_image_paths]\n",
    "test_embeddings = [get_embedding(img_path, model).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24729707-2d6a-45eb-9df9-19f323994ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Store data (serialize)\n",
    "with open('train_embeddings_mobilenet_v3_small.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('val_embeddings_mobilenet_v3_small.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_embeddings_mobilenet_v3_small.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3bfd575-a3c1-4018-b25e-c712b1051517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0525,  2.1514, -0.0647,  ...,  1.6340,  1.5746,  1.7384]),\n",
       " tensor([ 0.0904,  2.1105, -0.3276,  ...,  1.5987,  1.3902,  1.5455]),\n",
       " tensor([ 0.1437,  2.1723, -0.3820,  ...,  1.3958,  1.3506,  1.4212]),\n",
       " tensor([-0.1090,  1.8496, -0.2678,  ...,  1.2653,  1.1023,  1.1181]),\n",
       " tensor([ 0.1378,  2.2169, -0.3170,  ...,  1.1384,  1.2770,  1.2246])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1baad6-fe9b-4eb0-9849-d79122e067a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MobileNetV3 Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "501faa04-0e20-4b8b-9206-5dfc4887b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV3Large(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d79c403-7662-4afc-a1e2-557e05c9a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "868592c5-2813-456b-8c23-12e668d7547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b14b1-22d9-42aa-be6d-c26c4ffc2d1b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e15779-213d-479a-b9a1-0abbcd825da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'mobilenet_large_v3_ft_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e48cd6a-15fc-4468-88cb-0af099c4749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model.load_state_dict(torch.load('mobilenet_large_v3_epoch_22.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e8f9580-de65-4044-beaf-0af0d791c32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.load_state_dict(torch.load('mobilenet_large_v3_ft_epoch_5.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "077a896b-fe9c-4eee-a030-50cacbfa83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [get_embedding(img_path, model).cpu() for img_path in train_image_paths]\n",
    "val_embeddings = [get_embedding(img_path, model).cpu() for img_path in val_image_paths]\n",
    "test_embeddings = [get_embedding(img_path, model).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05807e50-fc05-448e-9d0d-174ec119712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Store data (serialize)\n",
    "with open('train_embeddings_mobilenet_v3_large.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('val_embeddings_mobilenet_v3_large.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_embeddings_mobilenet_v3_large.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48399b83-a193-4658-a069-e2c8103ef74b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EfficientNetB3 (with and without AUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d07df1a-c3df-4e57-a0ce-962d627f488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB3(num_classes=100, embedding_dim=4096).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f97fec2-90a6-4a2d-bb98-e552a0cbbe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63398038-5957-4546-ad46-0352b07e9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3656b8-faf5-4f32-b0d7-d2cb6cb922fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'efficientnet_b3_aug_ft_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc3bb632-7f21-4dcb-b659-5bb41de3cfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('efficientnet_b3_aug_epoch_67.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "231af86c-23af-40e1-989c-c866a981da97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('efficientnet_b3_aug_ft_epoch_3.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c908b53-0ecb-4ac3-822e-4d8ee9e182ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('efficientnet_b3_ftft_epoch_3.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6c31b7f-0825-4ecc-b15d-d0944df4386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.extend(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5fe16f1-2811-4197-b6be-a2f7ccd7085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 85, 41, 74, 33] 2000\n"
     ]
    }
   ],
   "source": [
    "preds = [pred.item() for pred in preds]\n",
    "print(preds[:5], len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0765368-62f2-4e24-b413-b8388af357fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 64.40%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(val_image_labels, preds)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34084004-cb61-40c2-a87f-7979d5eece02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 1288\n",
      "Number of Incorrect Predictions: 712\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(val_image_labels, preds)\n",
    "\n",
    "# Summing the diagonal elements gives the total number of correct predictions\n",
    "correct_predictions = np.trace(cm)\n",
    "total_predictions = cm.sum()\n",
    "\n",
    "print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Number of Incorrect Predictions: {total_predictions - correct_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236c590-cb8d-4551-afa4-7fb89151b6ed",
   "metadata": {},
   "source": [
    "### Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c84f9b1b-128f-49f5-a544-db33041afb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_image_paths, transform = val_transform)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "196cec4d-a8e8-446d-9e32-39ec2ac990fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for images in test_data_loader:  # Assuming val_loader is your validation DataLoader\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        # val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_preds.extend(predicted.cpu().int())\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "317883e2-7370-4a94-abf2-b90d0a342b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 16, 40, 13, 69]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = [pred.item() for pred in test_preds]\n",
    "test_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36ebdfc3-1af1-4afa-b238-a5f3c44641af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Class'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac614d43-e740-413e-9e85-8d9d31581759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046f61c4-b825-459a-8b2d-07503f5b94a5.png</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67db001f-e287-4950-ac49-6683b493d1a4.png</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ffef91a-aaf9-4d0d-a219-83a9f5282361.png</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c00af570-0000-4f8f-a3f2-c37a981bfdb1.png</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Image  Class\n",
       "0  046f61c4-b825-459a-8b2d-07503f5b94a5.png     67\n",
       "1  67db001f-e287-4950-ac49-6683b493d1a4.png     16\n",
       "2  9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png     40\n",
       "3  5ffef91a-aaf9-4d0d-a219-83a9f5282361.png     13\n",
       "4  c00af570-0000-4f8f-a3f2-c37a981bfdb1.png     69"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33d0ea76-873e-48f9-ad24-2fe9a0a8a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission_convv4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4f271-ad34-4a4e-b791-ba554ddf341f",
   "metadata": {},
   "source": [
    "### Generate Embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aabd1f51-3a99-41d4-8f54-70a36968188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [get_embedding(img_path, model).cpu() for img_path in train_image_paths]\n",
    "val_embeddings = [get_embedding(img_path, model).cpu() for img_path in val_image_paths]\n",
    "test_embeddings = [get_embedding(img_path, model).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef0627d1-02e9-451d-a719-76cc2def1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Store data (serialize)\n",
    "with open('train_embeddings_efficientnet_b3.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('val_embeddings_efficientnet_b3.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_embeddings_efficientnet_b3.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b9773-6485-42b0-a8f4-3859d74680b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Swin V2 T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c19834e-0421-432b-91c5-8578989dbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinV2TEncoder(embedding_dim=4096).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be611298-0646-4cf0-9032-4d8e8f9425ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170526f8-58d2-4460-b83a-5a608111a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8040fb-fc9e-4a1c-802b-52dcde6c193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 6 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'swin_v2_t_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00843b91-d5f8-4455-ab49-5c38271d56a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('swin_v2_t_epoch_31.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d3786-c2a1-40f8-94e7-df44d4ed597e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Efficient Net B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "871e0e39-c825-4eea-8240-f66807b5c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB4(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541a97d6-c0ec-498f-80c0-8fdff2e5186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "964d15ed-7c84-4fca-9638-0c4a555f56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4189a8-0288-4f44-ad48-92df19a26621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'efficientnet_b4_aug_ftft_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ea8d88b-2c89-480f-a8db-81e3fd14ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('efficientnet_b4_aug_epoch_67.pth')['model_state_dict'])\n",
    "# optimizer.load_state_dict(torch.load('efficientnet_b4_aug_epoch_67.pth')['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be014925-53cb-4005-a67b-633f7148d119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('efficientnet_b4_aug_ft_epoch_6.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80898efe-14bf-4f85-b693-d9dba82a1724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('efficientnet_b4_ftft_epoch_2.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc23979-eb6a-4b5f-9697-e44dd1e58ee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Efficient Net V2 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40a4928-3cc6-42f1-baca-c6c05b808e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetV2M(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a574c18-78a6-49c5-81b8-674eb417f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 64, 64).to(device) # Example input tensor (batch_size, channels, height, width)\n",
    "output = model(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67661f41-e155-4ff2-8867-011b5d43718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b54f5a2-2fda-477f-863a-ebddb9075073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop (optional, but recommended)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "       # Save checkpoint\n",
    "       checkpoint = {\n",
    "           'epoch': epoch + 1,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': val_loss,\n",
    "       }\n",
    "       torch.save(checkpoint, f'efficientnet_v2_m_ftft_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b8d470-2ff1-4c18-a482-c9133c5dc495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('efficientnet_v2_m_epoch_36.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad4cf27-acb3-4e7a-ba64-de532a206750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('efficientnet_v2_m_ft_epoch_5.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f80096ae-ab78-473f-bfc7-ee5feeb4a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [get_embedding(img_path, model).cpu() for img_path in train_image_paths]\n",
    "val_embeddings = [get_embedding(img_path, model).cpu() for img_path in val_image_paths]\n",
    "test_embeddings = [get_embedding(img_path, model).cpu() for img_path in test_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07e572c6-d6b6-4dd2-a753-b403f9afcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Store data (serialize)\n",
    "with open('train_embeddings_efficientnet_v2_m.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('val_embeddings_efficientnet_v2_m.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_embeddings_efficientnet_v2_m.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31957d-212b-4297-a9c9-6a717dcfa0b2",
   "metadata": {},
   "source": [
    "## Ensemble Voting\n",
    "\n",
    "#### Mobilenet\n",
    "- MobileNetV3Small (embedding generation using Cross Entropy Loss, 2048) -> validation accuracy is 60% after 2 rounds of  -> `mobilenet_v3_ft_epoch_6.pth`\n",
    "- MobileNetV3Large (embedding generation using Cross Entropy Loss, 2048) -> validation accuracy is 65% -> after 1 round of ft -> `mobilenet_large_v3_ft_epoch_5.pth`\n",
    "\n",
    "#### EfficientNet\n",
    "- EfficientNet B3 (embedding generation using CE, 2048) -> validation accuracy is 67% -> after 2 rounds of ft -> `efficientnet_b3_ftft_epoch_3` ----Kaggle 64%\n",
    "- EfficientNet V2 M (embedding generation using CE, 2048) -> validation accuracy is 64% -> after 2 rounds of ft -> `efficientnet_v2_m_ftft_epoch_3`\n",
    "- EfficientNet B4 (embedding generation using CE, 2048) -> validation accuracy is 64.7% -> after 2 rounds of ft -> ``\n",
    "- EfficientNet B4 with Aug (embedding generation using CE, 2048) -> validation accuracy is 57.7% -> after 2 rounds of ft\n",
    "- EfficientNet B3 With AUG (embedding generation using CE, 4096) -> validation accuracy is 65% -> after 1 round of ft (second round didn't change anything) -> `efficientnet_b3_aug_ft_epoch_3`\n",
    "\n",
    "#### Others\n",
    "- Max Vit T (without AUG) -> 45-50% after ft\n",
    "- Deep Encoder 52% after ft (WITH AUG)\n",
    "- Convnext Tiny (AUG) 1000 embedding and  -> 52%\n",
    "- Small(no AUG) 2048 embedding -> 50 (Convnexts converge very fast, but get stuck there)\n",
    "- **ConvV4 (no AUG) 1024 embedding -> 78.6% -> `convv4_ft_5` -> 75% Kaggle**\n",
    "- **ConvV3 (no AUG) 1024 embedding -> 78.4% (lost the 79.5) -> `convv3_ft_3` -> ?% Kaggle**\n",
    "- Conv V5 2048 -> 62%\n",
    "- **Conv V6 1024 -> 79.3% -> `convv6_ftft_epoch_3`**\n",
    "- \n",
    "#### Ensemble\n",
    "- MobileNetv3 large, EfficientNet B3, EfficientNet b4 -> 70.9% accuracy on validation, 68.2% Kaggle\n",
    "- MobileNetv3 large, EfficientNet B3, EfficientNet b4, efficient Net v2M -> 71.1 accuracy on validation, ? on Kaggle\n",
    "- MobileNetv3 samll, MobileNetv3 large, EfficientNet V3, EfficientNet b4, efficient Net v2M -> 70.1%, mobilenet v3 small decreases efficiency\n",
    "- MobileNetv3 large, EfficientNet B3, Efficient net B3 with Aug, EfficientNet b4, efficient Net v2M -> 73.7 accuracy on validation, 72 on Kaggle\n",
    "- MobileNetv3 large, EfficientNet B3, Efficient net B3 with Aug, EfficientNet b4, efficient Net v2M, convv4 -> 79.15 accuracy on validation -> Kaggle ?\n",
    "- MobileNetv3 large (65), EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4 (65), convv4 (79) -> 76.45 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), convv4 (70) -> 78.75 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4 (65), convv4 (79) -> 79.55 accuracy on validation **this is probably better than our current submission**\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M , convv4 (79) -> 79.3 accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M (65), convv3, convv4 (79) -> 81.15% accuracy on validation (80.25 now)\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), EfficientNet b4  (65), convv3, convv4 (79) -> 80.5% accuracy on validation\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), convv3, convv4 (79) -> 79.9 accuracy on validation\n",
    "- EfficientNet B3 (67), convv3, convv4 (79) -> 80.85% accuracy on validation\n",
    "- Efficient net B3 with Aug (65), convv3, convv4 (79) -> 80.85%\n",
    "- convv3, convv4 -> 79.55%\n",
    "- EfficientNet B3 (67), Efficient net B3 with Aug (65), efficient Net v2M  (65), convv3, convv4 (78), conv6 (79) -> 81.5% accuracy on validation\n",
    "- convv3, convv4 (78), conv6 (79) -> 81.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65adf6d2-92df-4ad2-9794-8132d062581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convv3 = ConvEncoderV3(embedding_dim=1024)\n",
    "convv3 = convv3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f741aa49-d055-4159-a31b-439f00807983",
   "metadata": {},
   "outputs": [],
   "source": [
    "convv4 = ConvEncoderV4(embedding_dim=1024)\n",
    "convv4 = convv4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e9bee23-4938-4f26-b46c-b17e6e7ee55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convv6 = ConvEncoderV6(embedding_dim=1024)\n",
    "convv6 = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6183468-c290-4dd1-b9f2-b21639cdd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv3 = MobileNetV3Large(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a2fd7ff-b6f6-4107-9143-13825934dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb3 = EfficientNetB3(num_classes=100, embedding_dim=2048).to(device)\n",
    "efficientnetb3_aug = EfficientNetB3(num_classes=100, embedding_dim=4096).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "110ab82e-a861-4da2-a1c7-4a8791ab36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb4 = EfficientNetB4(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a3e78ce-6564-4c10-925a-2b34db8f3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2m = EfficientNetV2M(num_classes=100, embedding_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f8eb0ad-e7e6-4c05-9e59-54fd2a1e3654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convv3.load_state_dict(torch.load('convv3_ftft_2_78.4.pth')['model_state_dict'])\n",
    "convv4.load_state_dict(torch.load('convv4_ft_5.pth')['model_state_dict'])\n",
    "convv6.load_state_dict(torch.load('convv6_ftft_epoch_3.pth')['model_state_dict'])\n",
    "# mobilenetv3.load_state_dict(torch.load('mobilenet_large_v3_ft_epoch_5.pth')['model_state_dict'])\n",
    "efficientnetb3.load_state_dict(torch.load('efficientnet_b3_ftft_epoch_3.pth')['model_state_dict'])\n",
    "efficientnetb3_aug.load_state_dict(torch.load('efficientnet_b3_aug_ft_epoch_3.pth')['model_state_dict'])\n",
    "efficientnetb4.load_state_dict(torch.load('efficientnet_b4_ftft_epoch_3.pth')['model_state_dict'])\n",
    "efficientnetv2m.load_state_dict(torch.load('efficientnet_v2_m_ftft_epoch_3.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c3713ab-6ff3-4034-9a0e-177f1d1580ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of models\n",
    "models = [convv6, convv3, convv4, efficientnetb3, efficientnetb3_aug, efficientnetv2m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62bdfd05-1110-4ebb-bca5-1b366f401b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.5\n"
     ]
    }
   ],
   "source": [
    "# Your validation loop\n",
    "preds = []\n",
    "total = 0\n",
    "correct = 0\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get ensemble predictions\n",
    "        ensemble_pred = ensemble_majority_voting(models, images)\n",
    "        preds.extend([pred.cpu().item() for pred in ensemble_pred])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (ensemble_pred.to(device) == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy and average loss if needed\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = val_loss / len(val_loader)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eecb0c4-6ce4-473b-b32e-6b18cadf130b",
   "metadata": {},
   "source": [
    "### Create test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32a6b411-4494-4115-85a6-f073a3cbbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_image_paths, transform = val_transform)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d55c3b1-3f74-4925-b195-44edf5300f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your validation loop\n",
    "test_preds = []\n",
    "total = 0\n",
    "correct = 0\n",
    "val_loss = 0\n",
    "total = 2000\n",
    "with torch.no_grad():\n",
    "    for images in test_data_loader:  # Assuming val_loader is your validation DataLoader\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Get ensemble predictions\n",
    "        ensemble_pred = ensemble_majority_voting(models, images)\n",
    "        test_preds.extend([pred.cpu().item() for pred in ensemble_pred])\n",
    "        # correct += (ensemble_pred.to(device) == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fb56503-b638-42de-bfba-b40b5ead7bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87a12f29-670f-422e-b196-1388e0383871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Class'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32ff9249-3072-49cc-9ef0-f0058d05dbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046f61c4-b825-459a-8b2d-07503f5b94a5.png</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67db001f-e287-4950-ac49-6683b493d1a4.png</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ffef91a-aaf9-4d0d-a219-83a9f5282361.png</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c00af570-0000-4f8f-a3f2-c37a981bfdb1.png</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Image  Class\n",
       "0  046f61c4-b825-459a-8b2d-07503f5b94a5.png     67\n",
       "1  67db001f-e287-4950-ac49-6683b493d1a4.png     91\n",
       "2  9f1d36a1-f046-4c5d-9e8a-0a3758ff605c.png     40\n",
       "3  5ffef91a-aaf9-4d0d-a219-83a9f5282361.png     13\n",
       "4  c00af570-0000-4f8f-a3f2-c37a981bfdb1.png     69"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "913b8429-ffd2-41d8-a8d4-69d1892deef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('ensemble_voting_b3_b3aug_v2m_cv3_cv4_cv6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d394c-2a36-4b08-9e48-f1eff2ed77ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
